import heapq
from pprintpp import pformat
from collections import OrderedDict
import dill # for pickling inner lambda...
from tqdm import tqdm
from loguru import logger
from itertools import chain
from os.path import expanduser
import shelve
import os
from frozendict import frozendict
from itertools import chain
from tabulate import tabulate
from snoop import snoop

from omni_converter.coconut.monad import try_monad, Success, Failure

data Edge(src,dst,f,cost,name="unnamed"):
    def __repr__(self):
        return f"Edge(<{self.src}> -> <{self.dst}> : {self.name})"
class NoRouteException(Exception)
class ConversionError(Exception)
class Conversion:
    def __init__(self,edges):
        self.edges = edges

    def __call__(self,x):
        init_x = x
        for e in self.edges:
            try:
                x = e.f(x)
            except Exception as ex:
                import inspect
                import pickle
                from loguru import logger
                logger.error(f"caught an exception. inspecting..{ex}")
                logger.warning(f"saving erroneous conversion for debug")
                info= dict(
                    start = self.edges[0].src,
                    end = self.edges[-1].dst,
                    x = init_x
                )

                logger.debug(f"conversion info = start:{info['start']},x:{info['x']}")
                p = os.path.abspath("last_erroneous_conversion.pkl")
                try:
                    with open(p,"wb") as f:
                        pickle.dump(info,f)
                except Exception as save_error:
                    from IPython import embed
                    logger.info(f"faield to dump failed conversions.")
                    raise save_error
                    #embed()
                source = inspect.getsource(e.f)
                logger.warning(f"saved last conversion error cause at {p}")
                raise ConversionError(f"exception in edge:{e.name} \n paths:{[e.name for e in self.edges]} \n x:{x} \n edge source:{source}") from ex
        return x

    def __getitem__(self,item):
        if isinstance(item,int):
            edges = [(self.edges[item])]
        else:
            edges = self.edges[item]
        return Conversion(edges)

    def __repr__(self):
        replace_pairs =[
            (" ",""),
            ("<frozendict","<fd"),
            ("'",""),
            ("type:","t:"),
            ("dtype","dt"),
            ("arrange","ar"),
            ("ch_rpr","ch"),
            ("v_range","vr"),
            ("ImageDef","ImDf")
        ]
        def replaces(tgt):
            tgt = repr(tgt)
            for a,b in replace_pairs:
                tgt = tgt.replace(a,b)
            return tgt
        if len(self.edges):
            start = self.edges[0].src
            end = self.edges[-1].dst
        else:
            start = "None"
            end = "None"
        path_data = [(e.name,(e.src |> replaces),(e.dst |> replaces)) for e in self.edges]
        """
        info = OrderedDict(
            name="Conversion",
            start = start,
            end = end,
            path = path_data,
            cost = sum([e.cost for e in self.edges])
        )
        """
        #print(tabulate(path_data))
        #return pformat(info)
        res = f"""Conversion:(cost={sum([e.cost for e in self.edges])})\n"""
        return res+tabulate(path_data)
    def trace(self,tgt):
        x = tgt
        for e in self.edges:
            x = e.f(x)
            yield dict(edge= e,x=x)

def new_conversion(edges):
    return Conversion(edges)
class _HeapContainer:
    def __init__(self,score,data):
        self.score = score
        self.data = data
    def __lt__(self,other):
        return self.score < other.score

def _astar(
          start,
          matcher,
          neighbors,
          max_depth = 100):
    """
    neighbors: node->[(mapper,next_node,cost,name)]
    """
    to_visit = []
    scores = dict()
    scores[start] = 0#heuristics(start,None)
    heapq.heappush(to_visit,
                   _HeapContainer(scores[start],(start,[])))
    visited = 0
    bar = tqdm(desc="solving with astar")
    while to_visit:
        bar.update(1)
        hc = heapq.heappop(to_visit)
        score = hc.score
        (pos,trace) = hc.data
        visited += 1
        #print(f"visit:{pos}")
        #print(f"{((trace[-1].a,trace[-1].name) if trace else 'no trace')}")
        #print(f"visit:{trace[-1] if trace else 'no trace'}")
        from loguru import logger
        if len(trace) >= max_depth: # terminate search on max_depth
            logger.error(f"search terminated due to max_depth")
            continue
        if matcher(pos): # reached a goal
            logger.debug(f"found after {visited} visits.")
            logger.debug(f"search result:\n{new_conversion(trace)}")
            bar.close()
            return trace
        for mapper,next_node,cost,name in neighbors(pos):
            assert isinstance(cost,int),f"cost is not a number. cost:{cost},name:{name},pos:{pos}"
            new_trace = trace + [Edge(pos,next_node,mapper,cost,name)]
            try:
                new_score = scores[pos] + cost #+ heuristics(next_node,end)
            except Exception as e:
                logger.error(f"pos:{pos},cost:{cost},next_node:{next_node}")
                raise e
            if next_node in scores and scores[next_node] <= new_score:
                continue

            else:
                scores[next_node] = new_score
                heapq.heappush(to_visit,_HeapContainer(new_score,(next_node,new_trace)))

    raise NoRouteException(f"no route found from {start} matching {matcher}")



def _astar_direct(
          start,
          end,
          neighbors,
          smart_neighbors,
          heuristics,
          edge_cutter,
          max_depth = 100,
          silent=False,
          debug_hook=None
          ):
    """
    neighbors: node->[(mapper,next_node,cost,name)]
    """
    from loguru import logger
    to_visit = []
    scores = dict()
    scores[start] = heuristics(start,end)
    heapq.heappush(to_visit,
                   _HeapContainer(scores[start],(start,[])))
    visited = 0
    if not silent:
        bar = tqdm(desc=f"solving with astar_direct from {start} to {end}")
    #logger.debug(f"starting search")
    last_bar_update = visited
    while to_visit:
        if visited - last_bar_update > 1000 and not silent:
            bar.update(visited-last_bar_update)
            last_bar_update = visited
        #logger.info(f"trying to heappop from {to_visit}")
        hc = heapq.heappop(to_visit)
        #logger.info(f"popped container:{hc}")
        score = hc.score
        #logger.info(f"got score:{score}")
        (pos,trace) = hc.data
        #logger.info(f"pos:{pos},trace:{trace}")
        #logger.info(f"{score}:{pos}")
        if not silent:
            #bar.write(f"score:{score}")
            pass

        #logger.debug(f"visiting:{pos}")
        visited += 1
        if len(trace) >= max_depth: # terminate search on max_depth
            #logger.info(f"terminate search on max depth")
            continue
        if pos == end: # reached a goal
            #logger.info(f"reached goal")
            if not silent:
                logger.debug(f"found after {visited} visits.")
                logger.debug(f"search result:\n{new_conversion(trace)}")
                bar.close()
            #logger.info(f"returning trace")
            return trace
        #logger.info(f"getting neighbors from {neighbors}")
        normal_nodes = list(neighbors(pos))
        #logger.info(f"getting smart neighbors")
        smart_nodes = list(smart_neighbors(pos,end))
        #logger.debug(f"{normal_nodes},{smart_nodes}")
        if visited % 10000 == 0 and not silent:
            msg = str(pos)[:50]
            bar.set_description(f"""pos:{msg:<50}""")
        for i,(mapper,next_node,cost,name) in enumerate(chain(normal_nodes,smart_nodes)):
            assert isinstance(cost,int),f"cost is not a number:{pos}"
            new_trace = trace + [Edge(pos,next_node,mapper,cost,name)]
            if debug_hook is not None:
                debug_hook(Conversion(new_trace))
            try:
                new_score = scores[pos] + cost + heuristics(next_node,end)
            except Exception as e:
                logger.error(f"pos:{pos},cost:{cost},next_node:{next_node}")
                raise e
            if next_node in scores and scores[next_node] <= new_score:
                continue
            elif(edge_cutter(pos,next_node,end)):
                continue
            else:
                scores[next_node] = new_score
                heapq.heappush(to_visit,_HeapContainer(new_score,(next_node,new_trace)))

    raise NoRouteException(f"no route found from {start} matching {end}. searched {visited} nodes.")

astar = (_astar ..> new_conversion) |> try_monad
astar_direct = (_astar_direct ..> new_conversion) |> try_monad

def _zero_heuristics(x,y):
    return 0

def _no_cutter(x,y,end):
    return False

MAX_MEMO = 2**(20)

class AStarSolver:
    """
    to make this picklable, you have to have these caches on global variable.
    use getstate and setstate
    actually, having to pickle this solver every time you pass auto_data is not feasible.
    so, lets have auto_data to hold solver in global variable and never pickle AStarSolver!.
    so forget about lru_cache pickling issues.
    """
    def __init__(self,
        rules=None,
        smart_rules=None,
        heuristics = _zero_heuristics,
        edge_cutter=_no_cutter,
        cache_path=os.path.join(expanduser("~"),".cache/autodata.shelve"),
        debug_hook=None
        ):
        from loguru import logger
        """
        rules: List[Rule]
        Rule: (state)->List[(converter:(data)->data,new_state,cost,conversion_name)]
        """
        self.rules = rules if rules is not None else []
        self.smart_rules = smart_rules if smart_rules is not None else []
        self.heuristics = heuristics
        self.edge_cutter = edge_cutter
        logger.debug(f"using autodata search cache at {cache_path}")
        self.debug_hook = debug_hook
        self.cache_path=cache_path
        self._init_non_picklable()

    def _init_non_picklable(self):
        from lru import LRU
        self.neighbors_memo = LRU(MAX_MEMO) # you cannot pickle a lru.LRU object, thus you cannot pickle this class for multiprocessing.
        self.search_memo = LRU(MAX_MEMO)
        self.smart_neighbors_memo = LRU(MAX_MEMO)
        self.direct_search_cache = DefaultShelveCache(lambda key:[(e.src,e.dst) for e in self._search_direct(key)],self.cache_path)
        self.direct_search_memo = LRU(MAX_MEMO)

    def __getstate__(self):
        return dict(
            rules = self.rules,
            smart_rules=self.smart_rules,
            heuristics = self.heuristics,
            edge_cutter=self.edge_cutter,
            debug_hook=self.debug_hook,
            cache_path=self.cache_path
        )
    def __setstate__(self,state):
        self.rules = state["rules"]
        self.smart_rules = state["smart_rules"]
        self.heuristics=state["heuristics"]
        self.edge_cutter=state["edge_cutter"]
        self.debug_hook=state["debug_hook"]
        self.cache_path=state["cache_path"]
        self._init_non_picklable()

    def neighbors(self,node):
        #logger.info(f"trying to get neighbors")
        if node in self.neighbors_memo:
            return self.neighbors_memo[node]

        res = []
        for rule in self.rules:
            #logger.info(f"checking rule:{rule}")
            edges = rule(node)
            #logger.info(f"got rules:{edges}")
            if edges is not None:
                 res += edges
        #logger.info(f"memorizing neighbors")
        self.neighbors_memo[node] = res
        #logger.info(f"memorized neighbors")
        return res
    def smart_neighbors(self,node,end):
        if (node,end) in self.smart_neighbors_memo:
            return self.smart_neighbors_memo[(node,end)]
        res = []
        for rule in self.smart_rules:
            edges = rule(node,end)
            if edges is not None:
                res += edges
        self.smart_neighbors_memo[(node,end)] = res
        #logger.debug(res)
        return res

    def invalidate_cache(self):
        self.neighbors_memo.clear()
        self.search_memo.clear()
        self.direct_search_memo.clear()

    def add_rule(self,f):
        self.rules.append(f)
        self.invalidate_cache()

    def search(self,start,matcher):
        #problem is that you can't hash matcher
        #let's use id of matcher for now.
        q = (start,id(matcher))
        if q in self.search_memo:
            res = self.search_memo[q]
        else:
            logger.debug(f"searching from {start} for matching {matcher}")
            res = astar(
                start=start,
                matcher=matcher,
                neighbors=self.neighbors,
            )
            self.search_memo[q] = res

        case res:
            match Success(res):
                return res
            match Failure(e,trc):
                raise e
    def _research_from_edges(self,edges):
        searched_edges = list(chain(*[self._search_direct((src,dst,True)).edges for src,dst in edges]))
        #logger.info(f"searched_edges:{searched_edges}")
        return Conversion(searched_edges)

    def _search_direct(self,q):
        # you cannot directly save the function.
        # so you need to save the paths and re-search it
        start,end,silent = q
        if not silent:
            logger.debug(f"searching {start} to {end}")
        res = astar_direct(
            start=start,
            end=end,
            neighbors=self.neighbors,
            smart_neighbors=self.smart_neighbors,
            heuristics=self.heuristics,
            edge_cutter=self.edge_cutter,
            silent=silent,
            debug_hook = self.debug_hook
        )
        case res:
            match Success(res):
                return res
            match Failure(e,trc):
                raise e

    def search_direct(self,start,end,silent=False):
        key = (start,end,silent)
        if key in self.direct_search_memo:
            return self.direct_search_memo[key]
        elif key in self.direct_search_cache:
            edges = self.direct_search_cache[key]
            conversion = self._research_from_edges(edges)
            self.direct_search_memo[key] = conversion
            #logger.debug(f"researched_conversion:\n{conversion}")
            return conversion
        else:
            conversion = self._search_direct(key)
            #logger.info(f"caching conversion:\n{conversion}")
            self.direct_search_cache[key] = [(e.src,e.dst) for e in conversion.edges]
            self.direct_search_memo[key] = conversion
            # I can memo every path in conversion actually.
            # however since the states are in a different space than a query, no speedups can be done.
            # if this astar knows about casting, it can first search for a cache though..

            return conversion

    def search_direct_any(self,start,ends):
        for cand in ends:
            try:
                res = self.search_direct(start,cand)
                return res
            except Exception as e:
                pass
        raise NoRouteException(f"no route found from {start} to any of {ends}")

